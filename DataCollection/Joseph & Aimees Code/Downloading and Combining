# Installing
!pip install wrds
!pip install import_ipynb

# Importing
import wrds
import pandas as pd
import os
# import import_ipynb
import glob
# from "/content/drive/MyDrive/Colab Notebooks/project_code.ipynb" import run_cleaning_pipeline
# Green does not work for Aimee. Can try on Joseph's computer later  ---> Because there's a space between Colab and Notbooks

from google.colab import drive
drive.mount('/content/drive')

# Connect to WRDS
wrds_db = wrds.Connection()
print("Connected to WRDS successfully!")

# Downloading and extracting list of countries
print("Downloading list of countries...")
countries = pd.read_excel('https://github.com/bkelly-lab/ReplicationCrisis/raw/master/GlobalFactors/Country%20Classification.xlsx')
countries_list = countries['excntry'].tolist()
print(f"Found {len(countries_list)} countries: {countries_list}")

relevant_chars = {"id":"id", "eom":"date", "excntry":"country", "gvkey":"gvkey",
                  "permno":"permno", "crsp_exchcd":"crsp_exch_code",
                  "comp_exchg":"comp_exch_code",
                  "me":"me", "sic":"sic", "ff49":"ff49",
                  "taccruals_at":"absacc", "oaccruals_at":"acc", "at_gr1":"agr",
                  "be_me":"bm", "cash_at":"cash_ass", "debt_at":"debt_ass",
                  "fcf_me":"cfp", "ret_6_0":"mom_6m", "ebit_sale":"pm",
                  "dolvol":"dolvol", "div12m_me":"dy", "be_gr1a":"egr",
                  "ni_me":"ep", "ami_126d":"ill", "ret_12_1":"mom_12m",
                  "at_be":"lev", "debtlt_gr1a":"lgr", "rmax1_21d":"maxret",
                  "ret_1_0":"rev_1m", "ret_6_1":"mom_6m_lag",
                  "taccruals_ni":"pctacc", "rvol_21d":"retvol", "ni_be":"roe",
                  "sale_gr1":"sgr", "sales":"sales", "prc":"price",
                  "turnover_126d":"turn"
                  }
rel_chars = []

for i in relevant_chars:
    rel_chars.append(i)

# Downloading the data
def download_wrds_data(
    countries_list,
    rel_chars,
    wrds_db,
    batch_size=400000,
    save_csv=True,
    file_path="/content/drive/MyDrive/Countries"
):
    """
    Downloads WRDS data in batches for each country and returns a combined DataFrame.
    Each country is removed from the list after it is fully processed.

    Parameters:
    - countries_list: List of country codes to download (e.g., ['USA', 'GBR', 'FRA'])
    - rel_chars: List of relevant WRDS variable names (e.g., ['bm', 'me', 'ep'])
    - wrds_db: Active WRDS connection object
    - batch_size: Number of rows to download per batch (default: 400,000).
    - save_csv: If True, saves each batch as a CSV file named "{country}_batch_{n}.csv"
    - file_path: Directory path where CSVs will be saved

    Returns:
    - full_df: Combined DataFrame of all downloaded data
    - remaining_countries: List of countries that were not processed (empty if successful)
    """

    all_data = []
    remaining_countries = countries_list.copy()

    print("Starting WRDS batch download")
    print(f"Batch size set to {batch_size:,} rows per batch.\n")

    os.makedirs(file_path, exist_ok=True)

    for country in countries_list.copy():
        print(f"\nProcessing country: {country}")
        offset = 0

        try:
            while True:
                print(f"Downloading batch {offset // batch_size + 1} (offset {offset}) for {country}...")

                sql_query = f"""
                SELECT {', '.join(rel_chars)}
                FROM contrib.global_factor
                WHERE common=1 AND exch_main=1 AND primary_sec=1 AND obs_main=1
                  AND excntry = '{country}'
                LIMIT {batch_size} OFFSET {offset}
                """

                data = wrds_db.raw_sql(sql_query)
                print(f"Retrieved {data.shape[0]} rows, {data.shape[1]} columns")

                if data.empty:
                    print(f"🏁 Finished downloading data for {country}.")
                    break

                # Save to CSV
                if save_csv:
                    filename = f"{country}_batch_{offset // batch_size}.csv"
                    file_path_with_filename = os.path.join(file_path, filename)
                    print(f"Saving batch to {file_path_with_filename}")
                    data.to_csv(file_path_with_filename, index=False)

                all_data.append(data)
                offset += batch_size

        except Exception as e:
            print(f"ERROR while processing {country}: {e}")
            continue

        # Remove processed country
        remaining_countries.remove(country)

    print("\nAll countries processed successfully.")
    print(f"Combining {len(all_data)} batches into final DataFrame...")

    if all_data:
        full_df = pd.concat(all_data, ignore_index=True)
        print(f"Final full_df shape: {full_df.shape}")
    else:
        full_df = pd.DataFrame()
        print("No data was collected!")

    return full_df, remaining_countries

df, remaining_countries = download_wrds_data(countries_list, rel_chars, wrds_db,
                                             batch_size=100000)

#-------------------------------------------- Code to be run separately -------------------------------------------------
# As the retrieval of data crashes before I can merge together, this function
# combines all the batches

def write_combined_csv_in_chunks(folder_path, output_path):
    all_files = glob.glob(os.path.join(folder_path, "*.csv"))
    print(f"Writing {len(all_files)} files into {output_path}...")

    first_file = True
    for f in all_files:
        df = pd.read_csv(f)
        print(f"Appending {f} with {df.shape[0]} rows...")

        df.to_csv(output_path, mode='a', index=False, header=first_file)
        first_file = False  # this means it only writes the header once

    print("Done writing combined CSV!")

# Preview of the merged CSVs
df_preview = pd.read_csv("/content/drive/MyDrive/Countries/full_dataset.csv", nrows=10)
print(df_preview)

#-------------------------------------------- Code to be run separately -------------------------------------------------
# need extra libraries to be imported for this section to work:
from collections import defaultdict
import re

# Let's have another option rather than combining every single csv. Instead,
# let's combine by country with one final csv for each country:
def extract_batch_number(filename):
    """Extract batch number from filename like 'IRL_batch_12.csv' → 12"""
    match = re.search(r'_batch_(\d+)\.csv$', filename)
    return int(match.group(1)) if match else -1  # put unsorted ones at the end

def write_country_csvs_by_batch(folder_path, output_folder):
    # Get all csv files
    all_files = glob.glob(os.path.join(folder_path, "*.csv"))

    # Group files by country code (first 3 letters of filename)
    country_files = defaultdict(list)
    for f in all_files:
        basename = os.path.basename(f)
        country_code = basename[:3]
        country_files[country_code].append(f)

    print(f"Found {len(country_files)} countries...")

    # Create output directory if it doesn't exist
    os.makedirs(output_folder, exist_ok=True)

    for country, file_list in country_files.items():
        # Sort files by batch number
        sorted_files = sorted(file_list, key=lambda f: extract_batch_number(os.path.basename(f)))

        # Combine all batches
        combined_df = pd.concat([pd.read_csv(f) for f in sorted_files], ignore_index=True)
        output_file = os.path.join(output_folder, f"{country}.csv")
        combined_df.to_csv(output_file, index=False)

        print(f"Saved {output_file} with {combined_df.shape[0]} rows from {len(sorted_files)} batch file(s)")

    print("Done writing per-country CSVs!")


#below, I removed "Colab Notebooks/" from the path:
write_country_csvs_by_batch(
    folder_path="/content/drive/MyDrive/Countries",
    output_folder="/content/drive/MyDrive/Countries"
)

#-------------------------------------------- Code to be run separately -------------------------------------------------
# Define function to take each country csv, read it to a df and clean it.

def clean_each_country_file(input_folder, output_folder,
                            cleaning_function=run_cleaning_pipeline):
    """
    Loads each country CSV, applies cleaning function, saves cleaned version.

    Parameters:
    - input_folder: path to country_combined CSVs (e.g., 'IRL_combined.csv')
    - output_folder: path to save cleaned versions
    - cleaning_function: a function like run_cleaning_pipeline(df) -> df_cleaned
    """

    # Create output folder if not exists
    os.makedirs(output_folder, exist_ok=True)

    # Loop over each country file
    for filename in os.listdir(input_folder):
        if filename.endswith(".csv"):
            country = filename[:3]
            filepath = os.path.join(input_folder, filename)
            print(f"Reading {filename}...")

            df = pd.read_csv(filepath)

            print(f"Cleaning {country} data...")
            df_cleaned = cleaning_function(df)

            out_path = os.path.join(output_folder, f"{country}_cleaned.csv")
            df_cleaned.to_csv(out_path, index=False)

            print(f"Saved cleaned data for {country} ({df_cleaned.shape[0]} rows)")

    print("All country files cleaned and saved.")

